{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torchvision.datasets as datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from misc_functions import preprocess_image, recreate_image, save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Number of GPUs available: ',torch.cuda.device_count())\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Data():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.train_set = datasets.CIFAR10(\n",
    "        root = './data',\n",
    "        train = True,\n",
    "        download = True,\n",
    "        transform = transforms.Compose([transforms.Resize(224), transforms.ToTensor()])\n",
    "        )\n",
    "\n",
    "        self.test_set = datasets.CIFAR10(\n",
    "        root= './data',\n",
    "        train= False,\n",
    "        download = True,\n",
    "        transform = transforms.Compose([transforms.Resize(224), transforms.ToTensor()])\n",
    "        )\n",
    "        \n",
    "    def get_data(self):\n",
    "        return [self.train_set, self.test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data = Data()\n",
    "train, test = data.get_data()\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size= 256, shuffle= True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size= 10000, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 3, 224, 224])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU(inplace)\n",
       "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace)\n",
       "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): ReLU(inplace)\n",
       "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (39): ReLU(inplace)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace)\n",
       "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.vgg16_bn(pretrained= True)\n",
    "model.to(device)\n",
    "model.classifier[6].out_features = 10\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for params in model.parameters():\n",
    "    count += 1\n",
    "    if count < 57:\n",
    "        params.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "def train(epoch, losses, net):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        losses.append(loss.item())\n",
    "        _, predicted = outputs.max(1)  #this returns values and labels\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        print(batch_idx, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "def test(net, test_losses ):\n",
    "    \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_losses.append(loss.item())\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            print('Loss: %.3f | Acc: %.3f%%'\n",
    "                  % (test_loss, 100.*correct/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "0 196 Loss: 9.664 | Acc: 0.781% (2/256)\n",
      "1 196 Loss: 9.645 | Acc: 0.391% (2/512)\n",
      "2 196 Loss: 9.657 | Acc: 0.260% (2/768)\n",
      "3 196 Loss: 9.595 | Acc: 0.195% (2/1024)\n",
      "4 196 Loss: 9.535 | Acc: 0.156% (2/1280)\n",
      "5 196 Loss: 9.490 | Acc: 0.260% (4/1536)\n",
      "6 196 Loss: 9.426 | Acc: 0.223% (4/1792)\n",
      "7 196 Loss: 9.379 | Acc: 0.195% (4/2048)\n",
      "8 196 Loss: 9.351 | Acc: 0.174% (4/2304)\n",
      "9 196 Loss: 9.306 | Acc: 0.234% (6/2560)\n",
      "10 196 Loss: 9.206 | Acc: 0.249% (7/2816)\n",
      "11 196 Loss: 9.125 | Acc: 0.228% (7/3072)\n",
      "12 196 Loss: 9.038 | Acc: 0.210% (7/3328)\n",
      "13 196 Loss: 8.967 | Acc: 0.251% (9/3584)\n",
      "14 196 Loss: 8.886 | Acc: 0.286% (11/3840)\n",
      "15 196 Loss: 8.797 | Acc: 0.293% (12/4096)\n",
      "16 196 Loss: 8.720 | Acc: 0.276% (12/4352)\n",
      "17 196 Loss: 8.633 | Acc: 0.391% (18/4608)\n",
      "18 196 Loss: 8.538 | Acc: 0.493% (24/4864)\n",
      "19 196 Loss: 8.451 | Acc: 0.723% (37/5120)\n",
      "20 196 Loss: 8.362 | Acc: 0.800% (43/5376)\n",
      "21 196 Loss: 8.264 | Acc: 0.977% (55/5632)\n",
      "22 196 Loss: 8.171 | Acc: 1.138% (67/5888)\n",
      "23 196 Loss: 8.074 | Acc: 1.351% (83/6144)\n",
      "24 196 Loss: 7.983 | Acc: 1.500% (96/6400)\n",
      "25 196 Loss: 7.889 | Acc: 1.623% (108/6656)\n",
      "26 196 Loss: 7.797 | Acc: 1.881% (130/6912)\n",
      "27 196 Loss: 7.701 | Acc: 2.093% (150/7168)\n",
      "28 196 Loss: 7.609 | Acc: 2.371% (176/7424)\n",
      "29 196 Loss: 7.519 | Acc: 2.565% (197/7680)\n",
      "30 196 Loss: 7.426 | Acc: 2.835% (225/7936)\n",
      "31 196 Loss: 7.331 | Acc: 3.174% (260/8192)\n",
      "32 196 Loss: 7.243 | Acc: 3.385% (286/8448)\n",
      "33 196 Loss: 7.151 | Acc: 3.676% (320/8704)\n",
      "34 196 Loss: 7.066 | Acc: 3.828% (343/8960)\n",
      "35 196 Loss: 6.981 | Acc: 4.015% (370/9216)\n",
      "36 196 Loss: 6.889 | Acc: 4.413% (418/9472)\n",
      "37 196 Loss: 6.802 | Acc: 4.770% (464/9728)\n",
      "38 196 Loss: 6.726 | Acc: 4.988% (498/9984)\n",
      "39 196 Loss: 6.653 | Acc: 5.205% (533/10240)\n",
      "40 196 Loss: 6.571 | Acc: 5.554% (583/10496)\n",
      "41 196 Loss: 6.496 | Acc: 5.841% (628/10752)\n",
      "42 196 Loss: 6.423 | Acc: 6.177% (680/11008)\n",
      "43 196 Loss: 6.350 | Acc: 6.445% (726/11264)\n",
      "44 196 Loss: 6.278 | Acc: 6.806% (784/11520)\n",
      "45 196 Loss: 6.210 | Acc: 7.125% (839/11776)\n",
      "46 196 Loss: 6.141 | Acc: 7.480% (900/12032)\n",
      "47 196 Loss: 6.072 | Acc: 7.837% (963/12288)\n",
      "48 196 Loss: 6.010 | Acc: 8.163% (1024/12544)\n",
      "49 196 Loss: 5.946 | Acc: 8.492% (1087/12800)\n",
      "50 196 Loss: 5.884 | Acc: 8.862% (1157/13056)\n",
      "51 196 Loss: 5.824 | Acc: 9.172% (1221/13312)\n",
      "52 196 Loss: 5.766 | Acc: 9.456% (1283/13568)\n",
      "53 196 Loss: 5.708 | Acc: 9.737% (1346/13824)\n",
      "54 196 Loss: 5.650 | Acc: 10.163% (1431/14080)\n",
      "55 196 Loss: 5.593 | Acc: 10.505% (1506/14336)\n",
      "56 196 Loss: 5.540 | Acc: 10.780% (1573/14592)\n",
      "57 196 Loss: 5.484 | Acc: 11.166% (1658/14848)\n",
      "58 196 Loss: 5.429 | Acc: 11.553% (1745/15104)\n",
      "59 196 Loss: 5.379 | Acc: 11.842% (1819/15360)\n",
      "60 196 Loss: 5.326 | Acc: 12.193% (1904/15616)\n",
      "61 196 Loss: 5.275 | Acc: 12.613% (2002/15872)\n",
      "62 196 Loss: 5.226 | Acc: 12.965% (2091/16128)\n",
      "63 196 Loss: 5.179 | Acc: 13.336% (2185/16384)\n",
      "64 196 Loss: 5.132 | Acc: 13.786% (2294/16640)\n",
      "65 196 Loss: 5.091 | Acc: 13.980% (2362/16896)\n",
      "66 196 Loss: 5.046 | Acc: 14.342% (2460/17152)\n",
      "67 196 Loss: 5.006 | Acc: 14.591% (2540/17408)\n",
      "68 196 Loss: 4.963 | Acc: 14.906% (2633/17664)\n",
      "69 196 Loss: 4.920 | Acc: 15.290% (2740/17920)\n",
      "70 196 Loss: 4.878 | Acc: 15.647% (2844/18176)\n",
      "71 196 Loss: 4.838 | Acc: 15.951% (2940/18432)\n",
      "72 196 Loss: 4.799 | Acc: 16.278% (3042/18688)\n",
      "73 196 Loss: 4.760 | Acc: 16.670% (3158/18944)\n",
      "74 196 Loss: 4.723 | Acc: 17.010% (3266/19200)\n",
      "75 196 Loss: 4.689 | Acc: 17.265% (3359/19456)\n",
      "76 196 Loss: 4.653 | Acc: 17.548% (3459/19712)\n",
      "77 196 Loss: 4.620 | Acc: 17.778% (3550/19968)\n",
      "78 196 Loss: 4.583 | Acc: 18.122% (3665/20224)\n",
      "79 196 Loss: 4.551 | Acc: 18.394% (3767/20480)\n",
      "80 196 Loss: 4.517 | Acc: 18.682% (3874/20736)\n",
      "81 196 Loss: 4.485 | Acc: 19.007% (3990/20992)\n",
      "82 196 Loss: 4.453 | Acc: 19.282% (4097/21248)\n",
      "83 196 Loss: 4.421 | Acc: 19.624% (4220/21504)\n",
      "84 196 Loss: 4.391 | Acc: 19.894% (4329/21760)\n",
      "85 196 Loss: 4.361 | Acc: 20.190% (4445/22016)\n",
      "86 196 Loss: 4.331 | Acc: 20.488% (4563/22272)\n",
      "87 196 Loss: 4.300 | Acc: 20.814% (4689/22528)\n",
      "88 196 Loss: 4.271 | Acc: 21.045% (4795/22784)\n",
      "89 196 Loss: 4.243 | Acc: 21.341% (4917/23040)\n",
      "90 196 Loss: 4.214 | Acc: 21.635% (5040/23296)\n",
      "91 196 Loss: 4.187 | Acc: 21.913% (5161/23552)\n",
      "92 196 Loss: 4.161 | Acc: 22.169% (5278/23808)\n",
      "93 196 Loss: 4.135 | Acc: 22.440% (5400/24064)\n",
      "94 196 Loss: 4.109 | Acc: 22.685% (5517/24320)\n",
      "95 196 Loss: 4.082 | Acc: 22.965% (5644/24576)\n",
      "96 196 Loss: 4.058 | Acc: 23.168% (5753/24832)\n",
      "97 196 Loss: 4.034 | Acc: 23.382% (5866/25088)\n",
      "98 196 Loss: 4.010 | Acc: 23.619% (5986/25344)\n",
      "99 196 Loss: 3.985 | Acc: 23.898% (6118/25600)\n",
      "100 196 Loss: 3.963 | Acc: 24.103% (6232/25856)\n",
      "101 196 Loss: 3.939 | Acc: 24.353% (6359/26112)\n",
      "102 196 Loss: 3.918 | Acc: 24.492% (6458/26368)\n",
      "103 196 Loss: 3.896 | Acc: 24.745% (6588/26624)\n",
      "104 196 Loss: 3.873 | Acc: 25.015% (6724/26880)\n",
      "105 196 Loss: 3.850 | Acc: 25.287% (6862/27136)\n",
      "106 196 Loss: 3.828 | Acc: 25.511% (6988/27392)\n",
      "107 196 Loss: 3.809 | Acc: 25.655% (7093/27648)\n",
      "108 196 Loss: 3.787 | Acc: 25.896% (7226/27904)\n",
      "109 196 Loss: 3.766 | Acc: 26.147% (7363/28160)\n",
      "110 196 Loss: 3.746 | Acc: 26.376% (7495/28416)\n",
      "111 196 Loss: 3.726 | Acc: 26.604% (7628/28672)\n",
      "112 196 Loss: 3.705 | Acc: 26.853% (7768/28928)\n",
      "113 196 Loss: 3.685 | Acc: 27.104% (7910/29184)\n",
      "114 196 Loss: 3.666 | Acc: 27.262% (8026/29440)\n",
      "115 196 Loss: 3.647 | Acc: 27.505% (8168/29696)\n",
      "116 196 Loss: 3.628 | Acc: 27.724% (8304/29952)\n",
      "117 196 Loss: 3.609 | Acc: 27.963% (8447/30208)\n",
      "118 196 Loss: 3.593 | Acc: 28.105% (8562/30464)\n",
      "119 196 Loss: 3.575 | Acc: 28.294% (8692/30720)\n",
      "120 196 Loss: 3.557 | Acc: 28.532% (8838/30976)\n",
      "121 196 Loss: 3.540 | Acc: 28.737% (8975/31232)\n",
      "122 196 Loss: 3.521 | Acc: 28.979% (9125/31488)\n",
      "123 196 Loss: 3.505 | Acc: 29.136% (9249/31744)\n",
      "124 196 Loss: 3.488 | Acc: 29.334% (9387/32000)\n",
      "125 196 Loss: 3.472 | Acc: 29.520% (9522/32256)\n",
      "126 196 Loss: 3.454 | Acc: 29.743% (9670/32512)\n",
      "127 196 Loss: 3.438 | Acc: 29.910% (9801/32768)\n",
      "128 196 Loss: 3.422 | Acc: 30.084% (9935/33024)\n",
      "129 196 Loss: 3.407 | Acc: 30.285% (10079/33280)\n",
      "130 196 Loss: 3.391 | Acc: 30.475% (10220/33536)\n",
      "131 196 Loss: 3.376 | Acc: 30.632% (10351/33792)\n",
      "132 196 Loss: 3.361 | Acc: 30.801% (10487/34048)\n",
      "133 196 Loss: 3.347 | Acc: 30.982% (10628/34304)\n",
      "134 196 Loss: 3.332 | Acc: 31.152% (10766/34560)\n",
      "135 196 Loss: 3.319 | Acc: 31.282% (10891/34816)\n",
      "136 196 Loss: 3.305 | Acc: 31.432% (11024/35072)\n",
      "137 196 Loss: 3.290 | Acc: 31.641% (11178/35328)\n",
      "138 196 Loss: 3.277 | Acc: 31.787% (11311/35584)\n",
      "139 196 Loss: 3.262 | Acc: 31.961% (11455/35840)\n",
      "140 196 Loss: 3.249 | Acc: 32.134% (11599/36096)\n",
      "141 196 Loss: 3.235 | Acc: 32.306% (11744/36352)\n",
      "142 196 Loss: 3.222 | Acc: 32.482% (11891/36608)\n",
      "143 196 Loss: 3.208 | Acc: 32.650% (12036/36864)\n",
      "144 196 Loss: 3.194 | Acc: 32.834% (12188/37120)\n",
      "145 196 Loss: 3.181 | Acc: 32.984% (12328/37376)\n",
      "146 196 Loss: 3.168 | Acc: 33.139% (12471/37632)\n",
      "147 196 Loss: 3.155 | Acc: 33.327% (12627/37888)\n",
      "148 196 Loss: 3.142 | Acc: 33.512% (12783/38144)\n",
      "149 196 Loss: 3.129 | Acc: 33.690% (12937/38400)\n",
      "150 196 Loss: 3.119 | Acc: 33.780% (13058/38656)\n",
      "151 196 Loss: 3.107 | Acc: 33.936% (13205/38912)\n",
      "152 196 Loss: 3.094 | Acc: 34.099% (13356/39168)\n",
      "153 196 Loss: 3.083 | Acc: 34.233% (13496/39424)\n",
      "154 196 Loss: 3.071 | Acc: 34.378% (13641/39680)\n",
      "155 196 Loss: 3.059 | Acc: 34.525% (13788/39936)\n",
      "156 196 Loss: 3.048 | Acc: 34.659% (13930/40192)\n",
      "157 196 Loss: 3.036 | Acc: 34.820% (14084/40448)\n",
      "158 196 Loss: 3.025 | Acc: 34.950% (14226/40704)\n",
      "159 196 Loss: 3.014 | Acc: 35.105% (14379/40960)\n",
      "160 196 Loss: 3.004 | Acc: 35.219% (14516/41216)\n",
      "161 196 Loss: 2.993 | Acc: 35.359% (14664/41472)\n",
      "162 196 Loss: 2.982 | Acc: 35.487% (14808/41728)\n",
      "163 196 Loss: 2.972 | Acc: 35.618% (14954/41984)\n",
      "164 196 Loss: 2.962 | Acc: 35.741% (15097/42240)\n",
      "165 196 Loss: 2.952 | Acc: 35.860% (15239/42496)\n",
      "166 196 Loss: 2.942 | Acc: 36.001% (15391/42752)\n",
      "167 196 Loss: 2.932 | Acc: 36.158% (15551/43008)\n",
      "168 196 Loss: 2.921 | Acc: 36.291% (15701/43264)\n",
      "169 196 Loss: 2.911 | Acc: 36.422% (15851/43520)\n",
      "170 196 Loss: 2.901 | Acc: 36.538% (15995/43776)\n",
      "171 196 Loss: 2.892 | Acc: 36.655% (16140/44032)\n",
      "172 196 Loss: 2.882 | Acc: 36.789% (16293/44288)\n",
      "173 196 Loss: 2.872 | Acc: 36.921% (16446/44544)\n",
      "174 196 Loss: 2.862 | Acc: 37.080% (16612/44800)\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "losses = []\n",
    "test_losses = []\n",
    "for epoch in range(epochs):\n",
    "    train(epoch, losses, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_loss = [np.log(x) for x in losses]\n",
    "plt.plot(log_loss)\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = nn.Sequential(nn.Linear(in_features= 25088, out_features= 4096 ),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout2d(),\n",
    "                                nn.Linear(in_features= 4096, out_features= 4096),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout2d(),\n",
    "                                nn.Linear(in_features= 4096, out_features= 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNNLayerVisualization():\n",
    "    \"\"\"\n",
    "        Produces an image that minimizes the loss of a convolution\n",
    "        operation for a specific layer and filter\n",
    "    \"\"\"\n",
    "    def __init__(self, model, selected_layer, selected_filter):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.selected_layer = selected_layer\n",
    "        self.selected_filter = selected_filter\n",
    "        self.conv_output = 0\n",
    "        # Create the folder to export images if not exists\n",
    "        if not os.path.exists('generated'):\n",
    "            os.makedirs('generated')\n",
    "\n",
    "    def hook_layer(self):\n",
    "        def hook_function(module, grad_in, grad_out):\n",
    "            # Gets the conv output of the selected filter (from selected layer)\n",
    "            print(grad_out[0, self.selected_filter].shape)\n",
    "            self.conv_output = grad_out[0, self.selected_filter]\n",
    "        # Hook the selected layer\n",
    "        self.model[self.selected_layer].register_forward_hook(hook_function)\n",
    "\n",
    "    def visualise_layer_with_hooks(self):\n",
    "        # Hook the selected layer\n",
    "        self.hook_layer()\n",
    "        # Generate a random image\n",
    "        #random_image = np.uint8(np.random.uniform(150, 180, (224, 224, 3)))\n",
    "        random_image = images[3].permute(2,1,0)\n",
    "        # Process image and return variable\n",
    "        print(\"random image \", random_image.shape)\n",
    "        processed_image = preprocess_image(random_image, False)\n",
    "        print(\"processed images size\", processed_image.shape)\n",
    "        # Define optimizer for the image\n",
    "        optimizer = Adam([processed_image], lr=0.1, weight_decay=1e-6)\n",
    "        for i in range(1, 145):\n",
    "            optimizer.zero_grad()\n",
    "            # Assign create image to a variable to move forward in the model\n",
    "            x = processed_image\n",
    "            for index, layer in enumerate(self.model):\n",
    "                # Forward pass layer by layer\n",
    "                # x is not used after this point because it is only needed to trigger\n",
    "                # the forward hook function\n",
    "                x = layer(x)\n",
    "                # Only need to forward until the selected layer is reached\n",
    "                if index == self.selected_layer:\n",
    "                    # (forward hook function triggered)\n",
    "                    break\n",
    "            # Loss function is the mean of the output of the selected layer/filter\n",
    "            # We try to minimize the mean of the output of that specific filter\n",
    "            loss = -torch.mean(self.conv_output)\n",
    "            print('Iteration:', str(i), 'Loss:', \"{0:.2f}\".format(loss.data))\n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            # Update image\n",
    "            optimizer.step()\n",
    "            # Recreate image\n",
    "            self.created_image = recreate_image(processed_image)\n",
    "            # Save image\n",
    "            if i % 5 == 0:\n",
    "                im_path = 'generated/layer_vis_l' + str(self.selected_layer) + \\\n",
    "                    '_f' + str(self.selected_filter) + '_iter' + str(i) + '.jpg'\n",
    "                save_image(self.created_image, im_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layer = 17\n",
    "filter_pos = 5\n",
    "# Fully connected layer is not needed\n",
    "pretrained_model = models.vgg16(pretrained=True).features\n",
    "layer_vis = CNNLayerVisualization(pretrained_model, cnn_layer, filter_pos)\n",
    "\n",
    "# Layer visualization with pytorch hooks\n",
    "layer_vis.visualise_layer_with_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"dim: \", images[3][0].shape)\n",
    "fig = plt.figure()\n",
    "plt.imshow(images[3][0], interpolation= 'bicubic')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " models.vgg16(pretrained=True).features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(layer_vis.created_image[:,:,0], interpolation= 'bicubic')\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model[17]\n",
    "images[1].permute(2,1,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
